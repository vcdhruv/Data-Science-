Series:
    series is one dimentional array like objects.
    data = [1,2,3,4,5]
    series = pd.Series(data)
    print("Series \n",series)
    this will give output as :
    Series
    0   1
    1   2
    2   3
    3   4
    4   5
    dtype: int64

Series From Dictionary:
    data = {'a':1,'b':2,'c':3}
    series = pd.Series(data)
    print("Series \n",series)
    this will give output as :
    a   1
    b   2
    c   3
    dtype: int64

Create Index Manual:
    data = [10,20,30]
    index = ['a','b','c']
    pd.Series(data,index=index)
    a    10
    b    20
    c    30
    dtype: int64

Dataframe:
    dataframe can multiple number of rows and columns.

Dataframe from Dictionary:
    data={
        'Name':['Krish','vishmay','noone'],
        'Age':[25,20,189],
        'City':['Rajkot','Banglore','Florida']
    }
    df = pd.DataFrame(data)

Dataframe from List Of Dictionaries:
    data = [
        {'Name':'vcd','Age':32,'City':'Rajkot'},
        {'Name':'mcd','Age':32,'City':'Rajkot'},
        {'Name':'cmd','Age':32,'City':'Rajkot'}
    ]
    df = pd.DataFrame(data)
    df

To read csv file:
    df = pd.read_csv('sales_data.csv')
    df
    To read top 5 rows :
        df.head(5)
    TO read last 5 records : 
        df.tail(5)
    
If we want to access row index : 
    df.loc[0]

If we want to access with index location :
    df.iloc[0]

If we want to access with key :
    data={
        'Name':['Krish','vishmay','noone'],
        'Age':[25,20,189],
        'City':['Rajkot','Banglore','Florida']
    }
    df = pd.DataFrame(data)

    df['name'] gives all names
    type(df['name']) gives series

If we want to access using at :
    df.at[1,'Age'] gives 20

If we want to access using iat :
    df.iat[2,2] gives 'Florida'

Adding a new column to df :
    df['salary'] = [50000,30000,20000]

Remove a column :
    df.drop(df['salary'],axis = 1,inplace=True) here axis = 1 means verical by default axis = 0

To add 1 to age :
    df['Age'] = af['Age'] + 1

TO remove row : 
    df.drop(0,inplace=True)

To get statistical summary : 
    df.describe()

To get datatypes :
    df.dtypes

df['status'] is a series.
df[['status']] is a dataframe.
df[['email','keywords','name']] to select multiple columns.
df.dtypes gives datatypes of all columns where object is string.
df1 = pd.read_excel("LUSID Excel - Setting up your market data.xlsx") to read excel file and it might have multiple sheets then we need to give sheet name to select specific sheet.
df3.to_csv("players_data.csv",index=False) to store in file and index = False means to remove row numbers to be stored.
pd.Series([4,8,15,16,23,42]) to create a series.
pd.Series(x,name="even_numbers") series with name given.
data = {
    "Name" : ["Alice","Bob","Claire"],
    "Age" : [25,30,27],
    "Gender" : ["Female","Male","Female"]
}
df = pd.DataFrame(data) to create dataframe.
Both Series and Dataframe are mutable in nature while panel is immutable.
d2 = {
    "Name" : pd.Series(["A","B","C"]),
    "Age" : pd.Series([21,22,23]),
    "Marks" : pd.Series([88,90,98])
}
df2 = pd.DataFrame(d2) to create dataframe from multiple series.
df.describe() will show statistical analysis of the numerical data and not string data.
df.dtypes[df.dtypes == 'object'] to select columns having datatype as object
df[['Ticket']][4:11:2] slicing operation in dataframe
df['new_col'] = 0 to create new column in dataframe.
pd.Categorical(df['Pclass']) will show unique categories of the data means it has [1,2,3] unique values.
df['Cabin'].unique() it will give unique records available in the dataframe
df[(df['Sex'] == 'female') & (df['Fare'] > df['Fare'].mean())] multiple conditions
df[['PassengerId','Survived','Pclass']][0:2] row and column intersections in dataframe
df.iloc[0:2,[['PassengerId','Survived','Pclass']]] it wont work because it takes default indexes given by python i.e row numbers.
df.iloc[0:2,[0,1,2]] it will work because we have given default indexes.
df.loc[0:2,[['PassengerId','Survived','Pclass']]] it will work because it takes named indexes.

Question : # Q2. Given a Pandas DataFrame df with columns 'A', 'B', and 'C', write a Python function to re-index the
# DataFrame with a new index that starts from 1 and increments by 2 for each row.

Solution : 
df1 = pd.DataFrame({
    'A':[1,2],
    'B':[3,4],
    "C":[5,6]
})
df1.reset_index(drop=True)
new_index=[x for x in range(1,2*len(df1) + 1,2)]
df1.index = new_index

The DataFrame.size attribute returns the total number of elements in the DataFrame.
It is calculated as the product of the number of rows and the number of columns in the DataFrame.
The returned value represents the total number of cells or elements in the DataFrame, including both data values and missing/NaN values.
The size attribute returns a single integer value.

The DataFrame.shape attribute returns a tuple that represents the dimensions of the DataFrame.
It returns the number of rows followed by the number of columns in the DataFrame.
The returned value provides information about the structure of the DataFrame, indicating its shape.
The shape attribute returns a tuple with two values.

pd.date_range(start='2023-06-21',periods=7) to generate the date range
df.drop('PassengerId',axis=1,inplace=True) to remove column from the DataFrame
df.drop(3,inplace=True) to remove row from the DataFrame.
df.set_index("Name",inplace = True) to set index manually.
df.reset_index() to get default index back.
df.dropna(inplace=True) to remove row where records are nan.
df.dropna(axis=1) to remove column from the DataFrame where records are nan.
df.fillna("vcd") is used to fill the records nan with "vcd"
g = df1.groupby('Survived') to group by
g.max() to view details
g.max().transpose() to transpose the data
pd.concat([f1,f2]) to concat vertically
pd.concat([f1,f2],axis1) to concat horizontally
pd.merge(data1,data2,how='outer') # full outer merge see advance_python_2.ipynb
data1.join(data2,how="outer") for join operation both data column name must be different otherwise it will overlap , it will be on indexes not on columns like merge
apply() is used to apply whole function on entire column
df1['Fare_RS'] = df1['Fare'].apply(lambda x : x*80) will be applied on entire column.
df1['Name_len'] = df1['Name'].apply(len)
def cat_fare(x):
    if x < 10:
        return "cheap"
    elif x >= 10 and x < 20:
        return "mid"
    else:
        return "high"
df1["Catergories_fare"] = df1['Fare'].apply(cat_fare)
df1.reindex(['b','c','d','a']) to reindex data
for i in df1.iterrows(): this will create generator object and will print first index then values of first row and so on. 
    print(i)  

df2 = df1[['a','b']]
df2.map(lambda x : x**2) this is used on entire dataframe while apply() is used on entire column
df1.sort_values("c") for sorting based on values
df1.sort_index() for sorting based on index
pd.set_option("display.max_colwidth",500) see its example in advance_pandas_3.ipynb
see window function in advance_pandas_3.ipynb
df3['a'].cumsum() for cummulative sum
pd.date_range(start="2023-04-23",end="2023-06-23") will generate dates in the given range
df6['updated_date'] = pd.to_datetime(df6['date']) to convert string to datetime
df6['year'] = df6['updated_date'].dt.year to get the year from the date which is in datetime format not in object format
df6['day'] = df6['updated_date'].dt.day for day
df6['month'] = df6['updated_date'].dt.month for month
pd.Timedelta(days=1,hours=5,minutes=45) it gives this Timedelta('1 days 05:45:00')
pd.Timestamp.now().year to get current year
pd.Timestamp.now().month to get current month

date = pd.to_datetime('2023-06-20')
td = pd.Timedelta(days=1)
date + td
all 3 lines output : Timestamp('2023-06-21 00:00:00')

data = ["a","b","c","d","e","f","a","b","f","e"]
pd.Categorical(data)
['a', 'b', 'c', 'd', 'e', 'f', 'a', 'b', 'f', 'e']
Categories (6, object): ['a', 'b', 'c', 'd', 'e', 'f'] it gives unique data from datasets

cat = pd.Categorical(data)
cat.value_counts()
a    2
b    2
c    1
d    1
e    2
f    2
Name: count, dtype: int64

data visualization:

import matplotlib
d = pd.Series([1,2,3,3,5,6,6,8])
d.plot()


df.rename(columns={'Date':'Sales Date'}) to rename column
df['value_new'] = df['Value'].astype(int) to change data type 
multiple_grouped = df.groupby('Region')['Value'].agg(['mean','sum','count']) aggregate multiple functions
df.to_json() will convert dataframe to json.
df.to_json(orient='index')
df.to_json(orien='records')
df_excel.to_pickel('df_excel') to save pickle file which will not be able to open
pd.read_pickel()